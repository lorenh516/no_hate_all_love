{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem import LancasterStemmer \n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle_functions as pf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and split data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in test.csv and train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pf.read_pickle(bucket_name='advancedml-koch-mathur-hinkson', filename='sub_train_df10_preprocessed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 60)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['split', 'cleaned_w_stopwords', 'cleaned_no_stem', 'cleaned_porter', 'cleaned_lancaster', 'bigrams_unstemmed',\n",
    "       'perc_upper', 'num_exclam', 'num_words', 'perc_stopwords',\n",
    "       'num_upper_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(drop_cols, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 49)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'target', 'comment_text', 'severe_toxicity', 'obscene',\n",
       "       'identity_attack', 'insult', 'threat', 'asian', 'atheist', 'bisexual',\n",
       "       'black', 'buddhist', 'christian', 'female', 'heterosexual', 'hindu',\n",
       "       'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability',\n",
       "       'jewish', 'latino', 'male', 'muslim', 'other_disability',\n",
       "       'other_gender', 'other_race_or_ethnicity', 'other_religion',\n",
       "       'other_sexual_orientation', 'physical_disability',\n",
       "       'psychiatric_or_mental_illness', 'transgender', 'white', 'created_date',\n",
       "       'publication_id', 'parent_id', 'article_id', 'rating', 'funny', 'wow',\n",
       "       'sad', 'likes', 'disagree', 'sexual_explicit',\n",
       "       'identity_annotator_count', 'toxicity_annotator_count',\n",
       "       'cleaned_w_stopwords_str', 'cleaned_no_stem_str', 'cleaned_porter_str',\n",
       "       'cleaned_lancaster_str'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new column called \"toxicity_category\" in the train data frame categorizing comments as toxic (\"1\") or non-toxic (\"0\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['toxicity_category'] = train.target.apply(lambda x: 1 if x > 0.5 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train.csv into training (80%) and validation sets (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/24147278/how-do-i-create-test-and-train-samples-from-one-dataframe-with-pandas\n",
    "msk = np.random.rand(len(train)) < 0.8\n",
    "train_set = train[msk]\n",
    "validation_set = train[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    75292\n",
      "1     4537\n",
      "Name: toxicity_category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_set.toxicity_category.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    19088\n",
      "1     1083\n",
      "Name: toxicity_category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(validation_set.toxicity_category.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create small sample (\"train_sample1\") from the train_set on which to run models.  Ensure that samples are iid by replacing after each draw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sample = train_set.sample(frac=0.2, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    75292\n",
      "1     4537\n",
      "Name: toxicity_category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_set.toxicity_category.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set = pd.read_csv('processed_sample_20_perc.csv')\n",
    "# train_set = train_set.sample(frac=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic = train_set[train_set.toxicity_category == 1]\n",
    "nontoxic = train_set[train_set.toxicity_category == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((79829, 50), (4537, 50), (75292, 50))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape, toxic.shape, nontoxic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping the dataset to be include an equal number of toxic and nontoxic samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarter = len(toxic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_df = train_set.sample(quarter*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    13611\n",
      "1     4537\n",
      "Name: toxicity_category, dtype: int64\n",
      "1    9074\n",
      "0    9074\n",
      "Name: toxicity_category, dtype: int64\n",
      "1    13611\n",
      "0     4537\n",
      "Name: toxicity_category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "prepared_25 = toxic.append(nontoxic.sample(len(toxic)*3))\n",
    "prepared_25 = prepared_25.sample(frac=1).reset_index(drop=True)\n",
    "print(prepared_25.toxicity_category.value_counts())\n",
    "\n",
    "prepared_50 = toxic.append(toxic).append(nontoxic.sample(len(toxic)*2))\n",
    "prepared_50 = prepared_50.sample(frac=1).reset_index(drop=True)\n",
    "print(prepared_50.toxicity_category.value_counts())\n",
    "\n",
    "prepared_75 = toxic.append(toxic).append(toxic).append(nontoxic.sample(len(toxic)))\n",
    "prepared_75 = prepared_75.sample(frac=1).reset_index(drop=True)\n",
    "print(prepared_75.toxicity_category.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model_df, train_perc=.80,  model_type = \"SVM\", \n",
    "             see_inside=False, comments=\"comment_text\",\n",
    "             target='toxicity_category'):\n",
    "    '''\n",
    "    This function runs a single machine learning model as per the specified parameters.\n",
    "    \n",
    "    Input(s):\n",
    "        model_df: source data frame\n",
    "        train_perc: percentage that should be used for training set\n",
    "        addtl_feats: (list) list of non text columns to include\n",
    "        model_type: which machine learning model to use\n",
    "        see_inside: returns the intermediate tokenized and vectorized arrays\n",
    "        comments: source column for text data\n",
    "        target: source column for y values\n",
    "        \n",
    "    Output(s):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    train_start = 0\n",
    "    train_end = round(model_df.shape[0]*train_perc) \n",
    "\n",
    "    test_start = train_end\n",
    "    test_end = model_df.shape[0]\n",
    "    \n",
    "    X_all = model_df[comments].values\n",
    "    y_all = model_df[target].values\n",
    "\n",
    "    # calculating frequencies\n",
    "    tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "    fitted_vectorizer=tfidf_vectorizer.fit(model_df[comments].values.astype('U'))\n",
    "    X_all_tfidf =  fitted_vectorizer.transform(model_df[comments].values.astype('U'))\n",
    "    \n",
    "    \n",
    "    X_train = X_all_tfidf[train_start:train_end]\n",
    "    y_train = model_df[train_start:train_end][target].values\n",
    "    y_train=y_train.astype('int')\n",
    "    \n",
    "\n",
    "    X_test = X_all_tfidf[test_start:test_end]\n",
    "    y_test = model_df[test_start:test_end][target].values\n",
    "    print(\"fitting model now\")\n",
    "    model_dict = {}\n",
    "    model_dict[\"MultiNB\"] = MultinomialNB()\n",
    "    model_dict[\"GaussNB\"] = GaussianNB()\n",
    "    model_dict['SVM'] = svm.SVC(kernel='linear', probability=True, random_state=1008)\n",
    "    model_dict[\"LR\"] = LogisticRegression(penalty=\"l1\",C=1e5)\n",
    "        \n",
    "    clf = model_dict[model_type].fit(X_train, y_train)\n",
    "    \n",
    "    predicted = clf.predict(X_test)\n",
    "    \n",
    "    output = model_df[test_start:test_end]\n",
    "    output['predicted'] = predicted\n",
    "    output['y_test'] = y_test\n",
    "    output['accuracy'] = output.predicted == output.y_test\n",
    "    \n",
    "    if see_inside == True:\n",
    "        return clf, output, X_all_counts, X_all_tfidf\n",
    "    else:\n",
    "        return clf, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(output, should_print=True, round_to=3):\n",
    "    metrics = {}\n",
    "    targets = output[output.y_test == 1]\n",
    "    nontargets = output[output.y_test == 0]\n",
    "    \n",
    "    dfs = [output, targets, nontargets]\n",
    "    labels = [\"Overall\", \"Target\", \"Non-Target\"]\n",
    "    \n",
    "    for i in range(len(dfs)):\n",
    "\n",
    "        df, label = dfs[i], labels[i]\n",
    "        if label == \"Non-Target\":\n",
    "            pos_label = 0\n",
    "        else:\n",
    "            pos_label = 1\n",
    "        \n",
    "        metrics[label] = {}\n",
    "        \n",
    "        \n",
    "        accuracy = round(accuracy_score(df.y_test, df.predicted), round_to)\n",
    "        metrics[label]['Accuracy'] = accuracy\n",
    "        \n",
    "        precision = round(precision_score(df.y_test, df.predicted, pos_label=pos_label), round_to)\n",
    "        metrics[label]['Precision'] = precision\n",
    "\n",
    "        recall = round(recall_score(df.y_test, df.predicted, pos_label=pos_label), round_to)\n",
    "        metrics[label]['Recall'] = recall\n",
    "        \n",
    "        f1 = round(f1_score(df.y_test, df.predicted, pos_label=pos_label), round_to)\n",
    "        metrics[label]['F1'] = f1\n",
    "\n",
    "        if label == \"Overall\":\n",
    "            roc_auc = round(roc_auc_score(df.y_test, df.predicted), round_to)\n",
    "            metrics[label]['ROC_AUC'] = roc_auc\n",
    "            \n",
    "        if should_print == True:\n",
    "            print(\"{} Accuracy: {}\".format(label, accuracy))\n",
    "            print(\"{} Precision: {}\".format(label, precision))\n",
    "            print(\"{} Recall: {}\".format(label, recall))\n",
    "            print(\"{} F1 Score: {}\".format(label, f1))\n",
    "            if label == \"Overall\":\n",
    "                print(\"ROC_AUC: {}\".format(label, roc_auc))\n",
    "            print()\n",
    "            \n",
    "    return metrics\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highlighted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf, output = run_model(prepared_50, comments = \"cleaned_no_stem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 2019-05-29 19:49:51.041184\n",
      "['random_df', 'cleaned_w_stopwords_str']\n",
      "fitting model now\n",
      "Overall Accuracy: 0.954, Target Accuracy: 0.214, Non-Target Accuracy: 0.997\n",
      "\n",
      "2. 2019-05-29 19:51:31.170383\n",
      "['random_df', 'cleaned_no_stem_str']\n",
      "fitting model now\n",
      "Overall Accuracy: 0.955, Target Accuracy: 0.244, Non-Target Accuracy: 0.997\n",
      "\n",
      "3. 2019-05-29 19:52:46.985737\n",
      "['random_df', 'cleaned_porter_str']\n",
      "fitting model now\n",
      "Overall Accuracy: 0.956, Target Accuracy: 0.284, Non-Target Accuracy: 0.996\n",
      "\n",
      "4. 2019-05-29 19:53:50.908204\n",
      "['random_df', 'cleaned_lancaster_str']\n",
      "fitting model now\n",
      "Overall Accuracy: 0.957, Target Accuracy: 0.279, Non-Target Accuracy: 0.997\n",
      "\n",
      "5. 2019-05-29 19:54:53.467464\n",
      "['prepared_50', 'cleaned_w_stopwords_str']\n",
      "fitting model now\n",
      "Overall Accuracy: 0.89, Target Accuracy: 0.884, Non-Target Accuracy: 0.896\n",
      "\n",
      "6. 2019-05-29 19:59:36.379812\n",
      "['prepared_50', 'cleaned_no_stem_str']\n",
      "fitting model now\n",
      "Overall Accuracy: 0.892, Target Accuracy: 0.884, Non-Target Accuracy: 0.9\n",
      "\n",
      "7. 2019-05-29 20:03:16.330194\n",
      "['prepared_50', 'cleaned_porter_str']\n",
      "fitting model now\n",
      "Overall Accuracy: 0.89, Target Accuracy: 0.877, Non-Target Accuracy: 0.903\n",
      "\n",
      "8. 2019-05-29 20:06:32.695171\n",
      "['prepared_50', 'cleaned_lancaster_str']\n",
      "fitting model now\n",
      "Overall Accuracy: 0.878, Target Accuracy: 0.859, Non-Target Accuracy: 0.897\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_metric = 0\n",
    "metric_dict = ''\n",
    "model_factors = []\n",
    "\n",
    "SUBSET_OF_INTEREST = \"Target\"\n",
    "METRIC_OF_INTEREST = \"F1\"\n",
    "\n",
    "dfs = [random_df,  prepared_50]\n",
    "label = [\"random_df\", \"prepared_50\"]\n",
    "\n",
    "mn = 0\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    for text in ['cleaned_w_stopwords_str', 'cleaned_no_stem_str', 'cleaned_porter_str',\n",
    "       'cleaned_lancaster_str']:\n",
    "\n",
    "        factors = [label[i], text]\n",
    "        mn += 1\n",
    "        print(\"{}. {}\".format(mn, datetime.datetime.now()))\n",
    "        print(factors)\n",
    "\n",
    "        clf, output = run_model(dfs[i], comments = text)\n",
    "        metrics = get_metrics(output, should_print=False)\n",
    "        metric_of_interest = metrics[SUBSET_OF_INTEREST][METRIC_OF_INTEREST]\n",
    "        \n",
    "        print(\"Overall Accuracy: {}, Target Accuracy: {}, Non-Target Accuracy: {}\".format(metrics[\"Overall\"][\"Accuracy\"], metrics[\"Target\"][\"Accuracy\"], metrics[\"Non-Target\"][\"Accuracy\"]))\n",
    "        print() \n",
    "        \n",
    "        if (metric_of_interest > best_metric) and metric_of_interest < 0.95:\n",
    "            best_metric = metric_of_interest\n",
    "            \n",
    "            model_factors = factors\n",
    "            metric_dict = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['prepared_50', 'cleaned_w_stopwords_str'], 0.939)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_factors, best_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Overall': {'Accuracy': 0.89,\n",
       "  'Precision': 0.895,\n",
       "  'Recall': 0.884,\n",
       "  'F1': 0.889,\n",
       "  'ROC_AUC': 0.89},\n",
       " 'Target': {'Accuracy': 0.884, 'Precision': 1.0, 'Recall': 0.884, 'F1': 0.939},\n",
       " 'Non-Target': {'Accuracy': 0.896,\n",
       "  'Precision': 1.0,\n",
       "  'Recall': 0.896,\n",
       "  'F1': 0.945}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
