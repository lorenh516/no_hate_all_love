{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem import LancasterStemmer \n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and split data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in test.csv and train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"s3://advancedml-koch-mathur-hinkson/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"s3://advancedml-koch-mathur-hinkson/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new column called \"toxicity_category\" in the train data frame categorizing comments as toxic (\"1\") or non-toxic (\"0\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['toxicity_category'] = train.target.apply(lambda x: 1 if x > 0.5 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train.csv into training (80%) and validation sets (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/24147278/how-do-i-create-test-and-train-samples-from-one-dataframe-with-pandas\n",
    "msk = np.random.rand(len(train)) < 0.8\n",
    "train_set = train[msk]\n",
    "validation_set = train[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_set.toxicity_category.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validation_set.toxicity_category.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create small sample (\"train_sample1\") from the train_set on which to run models.  Ensure that samples are iid by replacing after each draw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = train_set.sample(frac=0.2, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_sample.toxicity_category.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = LancasterStemmer()\n",
    "ps = PorterStemmer() \n",
    "\n",
    "sw = set(stopwords.words('english'))\n",
    "sw.add('')\n",
    "\n",
    "def clean_text(text, stemming=None, remove_sw = True):\n",
    "    '''\n",
    "    This auxiliary function cleans text.\n",
    "    \n",
    "    Methods used for cleaning are: \n",
    "        (1) transform string of text to list of words,\n",
    "        (2) cleaned (lowercase, remove punctuation) and remove stop words,\n",
    "        (3) Porter stemming of cleaned (lowercase, remove punctuation) text, \n",
    "        (4) Lancaster stemming of cleaned (lowercase, remove punctuation), \n",
    "        (5) cleaned (lowercase, remove punctuation) without removing stop words.\n",
    "    \n",
    "    Inputs:\n",
    "        text (string) - A string of text.\n",
    "        stemming (parameter) - either Porter or Lancaster stemming method\n",
    "        remove_sw (boolean) - True/False remove stop words\n",
    "    \n",
    "    Outputs:\n",
    "        Cleaned text per the input parameters.\n",
    "    '''\n",
    "\n",
    "    t = text.replace(\"-\", \" \").split(\" \")\n",
    "    \n",
    "    t = [w.lower() for w in t]\n",
    "    \n",
    "    if remove_sw == True:\n",
    "        t = [w for w in t if w not in sw]\n",
    "    \n",
    "    if stemming == None:\n",
    "        pass;\n",
    "    elif stemming == \"Porter\":\n",
    "        t = [ps.stem(w) for w in t]\n",
    "    elif stemming == \"Lancaster\":\n",
    "        t = [ls.stem(w) for w in t]\n",
    "    else:\n",
    "        print(\"Please enter a valid stemming type\")\n",
    "        \n",
    "    t = [w.strip(string.punctuation) for w in t]\n",
    "\n",
    "    return ' '.join(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text_cleaning_cols(df):\n",
    "    '''\n",
    "    This function generates features and adds them to the data frame.\n",
    "    \n",
    "    Input:\n",
    "        Data frame with raw text strings.\n",
    "        \n",
    "    Output:\n",
    "        Data frame with added columns:\n",
    "            (1) 'split' - (list) Transforms the string of text into a list of words\n",
    "            (2) 'cleaned_w_stopwords' - (string) A string of text where words have been lowercased, \n",
    "                                        punctuation is removed, and stop words are removed\n",
    "            (3) 'cleaned_no_stem' - (string) A string of text where words have been lowercased, and \n",
    "                                        punctuation is removed (stop words remain in text).\n",
    "                                        \n",
    "            \n",
    "            (4) 'cleaned_porter' - (string) A string of text where words have been stemmed using the \n",
    "                                        Porter method on cleaned (lowercase, remove punctuation) text. \n",
    "            (5) 'cleaned_lancaster' - (string) A string of text where words have been stemmed using the\n",
    "                                        Lancaster method on cleaned (lowercase, remove punctuation) text.\n",
    "            (6) 'perc_upper' - (float) Percent of uppercase letters in the string of text.\n",
    "            (7) 'num_exclam' - (integer) Number of times an exclamation point appears in text.\n",
    "            (8) 'num_words' - (integer) Number of words in text.\n",
    "            \n",
    "    '''\n",
    "    print(datetime.datetime.now())\n",
    "    \n",
    "    df['split'] = df[\"comment_text\"].apply(lambda x: x.split(\" \"))\n",
    "    df['cleaned_w_stopwords'] = df[\"comment_text\"].apply(clean_text,args=(None,False),)\n",
    "\n",
    "    print(datetime.datetime.now())\n",
    "    df['cleaned_no_stem'] = df[\"comment_text\"].apply(clean_text,)\n",
    "    df['cleaned_porter'] = df[\"comment_text\"].apply(clean_text,args=(\"Porter\",),)\n",
    "    df['cleaned_lancaster'] = df[\"comment_text\"].apply(clean_text,args=(\"Lancaster\",),)\n",
    "\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    df['perc_upper'] = df[\"comment_text\"].apply(lambda x: round((len(re.findall(r'[A-Z]',x)) / len(x)), 3))\n",
    "\n",
    "    df['num_exclam'] = df[\"comment_text\"].apply(lambda x:(len(re.findall(r'!',x))))\n",
    "    \n",
    "    df['num_words'] = df[\"split\"].apply(lambda x: len(x))\n",
    "    print(\"DONE @ \" + datetime.datetime.now())\n",
    "        \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_text_cleaning_cols(train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_sample.to_csv('processed_sample_20_perc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle the dataset and send to s3 bucket:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "pickle_buffer = io.BytesIO()\n",
    "s3_resource = boto3.resource('s3')\n",
    "bucket='advancedml-koch-mathur-hinkson'\n",
    "key='preprocessed_train_sample_50pct.pkl'\n",
    "\n",
    "# test.to_pickle(pickle_buffer)\n",
    "# s3_resource.Object(bucket, 'full_preprocessed_test.pkl').put(Body=pickle_buffer.getvalue())\n",
    "\n",
    "test.to_pickle(key)\n",
    "s3_resource.Object(bucket,key).put(Body=open(key, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = pd.read_csv('processed_sample_20_perc.csv')\n",
    "train_sample = train_sample.sample(frac=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic = train_sample[train_sample.toxicity_category == 1]\n",
    "nontoxic = train_sample[train_sample.toxicity_category == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((144346, 55), (8466, 55), (135880, 55))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample.shape, toxic.shape, nontoxic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping the dataset to be include an equal number of toxic and nontoxic samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarter = len(toxic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_df = train_sample.sample(quarter*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    25398\n",
      "1     8466\n",
      "Name: toxicity_category, dtype: int64\n",
      "1    16932\n",
      "0    16932\n",
      "Name: toxicity_category, dtype: int64\n",
      "1    25398\n",
      "0     8466\n",
      "Name: toxicity_category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "prepared_25 = toxic.append(nontoxic.sample(len(toxic)*3))\n",
    "prepared_25 = prepared_25.sample(frac=1).reset_index(drop=True)\n",
    "print(prepared_25.toxicity_category.value_counts())\n",
    "\n",
    "prepared_50 = toxic.append(toxic).append(nontoxic.sample(len(toxic)*2))\n",
    "prepared_50 = prepared_50.sample(frac=1).reset_index(drop=True)\n",
    "print(prepared_50.toxicity_category.value_counts())\n",
    "\n",
    "prepared_75 = toxic.append(toxic).append(toxic).append(nontoxic.sample(len(toxic)))\n",
    "prepared_75 = prepared_75.sample(frac=1).reset_index(drop=True)\n",
    "print(prepared_75.toxicity_category.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model_df, train_perc=.80,  model_type = \"SVM\", \n",
    "             see_inside=False, comments=\"comment_text\",\n",
    "             target='toxicity_category'):\n",
    "    '''\n",
    "    This function runs a single machine learning model as per the specified parameters.\n",
    "    \n",
    "    Input(s):\n",
    "        model_df: source data frame\n",
    "        train_perc: percentage that should be used for training set\n",
    "        addtl_feats: (list) list of non text columns to include\n",
    "        model_type: which machine learning model to use\n",
    "        see_inside: returns the intermediate tokenized and vectorized arrays\n",
    "        comments: source column for text data\n",
    "        target: source column for y values\n",
    "        \n",
    "    Output(s):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    train_start = 0\n",
    "    train_end = round(model_df.shape[0]*train_perc) \n",
    "\n",
    "    test_start = train_end\n",
    "    test_end = model_df.shape[0]\n",
    "    \n",
    "    X_all = model_df[comments].values\n",
    "    y_all = model_df[target].values\n",
    "\n",
    "    # calculating frequencies\n",
    "    tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "    fitted_vectorizer=tfidf_vectorizer.fit(model_df[comments].values.astype('U'))\n",
    "    X_all_tfidf =  fitted_vectorizer.transform(model_df[comments].values.astype('U'))\n",
    "    \n",
    "    \n",
    "    X_train = X_all_tfidf[train_start:train_end]\n",
    "    y_train = model_df[train_start:train_end][target].values\n",
    "    y_train=y_train.astype('int')\n",
    "    \n",
    "\n",
    "    X_test = X_all_tfidf[test_start:test_end]\n",
    "    y_test = model_df[test_start:test_end][target].values\n",
    "    print(\"fitting model now\")\n",
    "    model_dict = {}\n",
    "    model_dict[\"MultiNB\"] = MultinomialNB()\n",
    "    model_dict[\"GaussNB\"] = GaussianNB()\n",
    "    model_dict['SVM'] = svm.SVC(kernel='linear', probability=True, random_state=1008)\n",
    "    model_dict[\"LR\"] = LogisticRegression(penalty=\"l1\",C=1e5)\n",
    "        \n",
    "    clf = model_dict[model_type].fit(X_train, y_train)\n",
    "    \n",
    "    predicted = clf.predict(X_test)\n",
    "    \n",
    "    output = model_df[test_start:test_end]\n",
    "    output['predicted'] = predicted\n",
    "    output['y_test'] = y_test\n",
    "    output['accuracy'] = output.predicted == output.y_test\n",
    "    \n",
    "    if see_inside == True:\n",
    "        return clf, output, X_all_counts, X_all_tfidf\n",
    "    else:\n",
    "        return clf, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(output, should_print=True, round_to=3):\n",
    "    metrics = {}\n",
    "    targets = output[output.y_test == 1]\n",
    "    nontargets = output[output.y_test == 0]\n",
    "    \n",
    "    dfs = [output, targets, nontargets]\n",
    "    labels = [\"Overall\", \"Target\", \"Non-Target\"]\n",
    "    \n",
    "    for i in range(len(dfs)):\n",
    "\n",
    "        df, label = dfs[i], labels[i]\n",
    "        if label == \"Non-Target\":\n",
    "            pos_label = 0\n",
    "        else:\n",
    "            pos_label = 1\n",
    "        \n",
    "        metrics[label] = {}\n",
    "        \n",
    "        \n",
    "        accuracy = round(accuracy_score(df.y_test, df.predicted), round_to)\n",
    "        metrics[label]['Accuracy'] = accuracy\n",
    "        \n",
    "        precision = round(precision_score(df.y_test, df.predicted, pos_label=pos_label), round_to)\n",
    "        metrics[label]['Precision'] = precision\n",
    "\n",
    "        recall = round(recall_score(df.y_test, df.predicted, pos_label=pos_label), round_to)\n",
    "        metrics[label]['Recall'] = recall\n",
    "        \n",
    "        f1 = round(f1_score(df.y_test, df.predicted, pos_label=pos_label), round_to)\n",
    "        metrics[label]['F1'] = f1\n",
    "\n",
    "        if label == \"Overall\":\n",
    "            roc_auc = round(roc_auc_score(df.y_test, df.predicted), round_to)\n",
    "            metrics[label]['ROC_AUC'] = roc_auc\n",
    "            \n",
    "        if should_print == True:\n",
    "            print(\"{} Accuracy: {}\".format(label, accuracy))\n",
    "            print(\"{} Precision: {}\".format(label, precision))\n",
    "            print(\"{} Recall: {}\".format(label, recall))\n",
    "            print(\"{} F1 Score: {}\".format(label, f1))\n",
    "            if label == \"Overall\":\n",
    "                print(\"ROC_AUC: {}\".format(label, roc_auc))\n",
    "            print()\n",
    "            \n",
    "    return metrics\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highlighted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf, output = run_model(prepared_50, comments = \"cleaned_no_stem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_metric = 0\n",
    "metric_dict = ''\n",
    "model_factors = []\n",
    "\n",
    "SUBSET_OF_INTEREST = \"Target\"\n",
    "METRIC_OF_INTEREST = \"F1\"\n",
    "\n",
    "dfs = [random_df,  prepared_50]\n",
    "label = [\"random_df\", \"prepared_50\"]\n",
    "\n",
    "mn = 0\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    for text in ['cleaned_w_stopwords', 'cleaned_no_stem', 'cleaned_porter', 'cleaned_lancaster']:\n",
    "\n",
    "        factors = [label[i], text]\n",
    "        mn += 1\n",
    "        print(\"{}. {}\".format(mn, datetime.datetime.now()))\n",
    "        print(factors)\n",
    "\n",
    "        clf, output = run_model(dfs[i], comments = text)\n",
    "        metrics = get_metrics(output, should_print=False)\n",
    "        metric_of_interest = metrics[SUBSET_OF_INTEREST][METRIC_OF_INTEREST]\n",
    "        \n",
    "        print(\"Overall Accuracy: {}, Target Accuracy: {}, Non-Target Accuracy: {}\".format(metrics[\"Overall\"][\"Accuracy\"], metrics[\"Target\"][\"Accuracy\"], metrics[\"Non-Target\"][\"Accuracy\"]))\n",
    "        print() \n",
    "        \n",
    "        if (metric_of_interest > best_metric) and metric_of_interest < 0.95:\n",
    "            best_metric = metric_of_interest\n",
    "            \n",
    "            model_factors = factors\n",
    "            metric_dict = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_factors, best_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
